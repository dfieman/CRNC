{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfieman/CRNC/blob/main/LandslideConcFinder_v2b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVnvUOnaIW6j"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from osgeo import gdal\n",
        "from scipy import interpolate, LowLevelCallable\n",
        "import scipy.integrate as integrate\n",
        "import multiprocessing\n",
        "import sys\n",
        "import datetime\n",
        "from functools import partial\n",
        "import os\n",
        "import glob\n",
        "import ctypes\n",
        "import numba\n",
        "from numba import jit\n",
        "import itertools\n",
        "#import more_itertools as mit\n",
        "from scipy.interpolate import Rbf\n",
        "from scipy.interpolate import griddata\n",
        "import random\n",
        "from matplotlib.lines import Line2D\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LMtL6Egt1lnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcnebHITIxuF"
      },
      "outputs": [],
      "source": [
        "#############\n",
        "# FUNCTIONS #\n",
        "#############\n",
        "\n",
        "# Open rasters as arrays\n",
        "def gdal_open(DEM_name,folder):\n",
        "    try:\n",
        "        raster = gdal.Open(folder+DEM_name)\n",
        "        nodata = raster.GetRasterBand(1).GetNoDataValue()\n",
        "        array = raster.GetRasterBand(1).ReadAsArray()\n",
        "        array[array==nodata]=np.nan\n",
        "        return array\n",
        "    except:\n",
        "        print(\"Could not open:\",folder+DEM_name)\n",
        "        return None\n",
        "\n",
        "\n",
        "# Slices the landslide depth every D50 until values in array equal 0. Rounds surface so divisible by the slicer length\n",
        "\n",
        "# Slices the landslide depth every D50 until values in array equal 0. Rounds surface so divisible by the slicer length\n",
        "def depthSlicer(LandslideArray,D50):\n",
        "    depth = [LandslideArray.copy()]\n",
        "    while np.any(LandslideArray >= 0.0):\n",
        "        LandslideArray = np.round(LandslideArray/D50)*D50 #rounds so divisible by D50\n",
        "        positive_values = LandslideArray > 0.0\n",
        "        LandslideArray[positive_values] -= D50\n",
        "        depth.append(LandslideArray.copy())#copy required due to odd behaviour\n",
        "        LandslideArray[LandslideArray==0.0]=np.nan #so to not have a zero value more than once\n",
        "    return depth\n",
        "\n",
        "# PRODUCTION FUNCTIONS #PRES\n",
        "\n",
        "#Convert elevation to units of hPa and g/cm^2. Input h = elevation [m] 2D array. Outputs pressure in hPa and g/cm2\n",
        "def h_units(h):\n",
        "    #Constants for pressure\n",
        "    Psl = 1013.25 #hPa; atmopheric pressure at sea level\n",
        "    tempsl = 288.15 #K; temperature at sea level\n",
        "    dTdz = 0.0065 #K/m; adiabatic lapse rate\n",
        "    g = 9.80665 #m/s^2\n",
        "    air = 0.0289644 #kg/mol; molecular mass of air\n",
        "    gas = 8.31446 #J/mol/K; gas constant\n",
        "    baro = Psl * np.exp(-1*(g*air/(gas*dTdz))*(np.log(tempsl)-np.log(tempsl-(dTdz*h))))\n",
        "    atPres = 1.019716*(1013.25-baro)\n",
        "    return baro, atPres\n",
        "\n",
        "## Scaling production functions:\n",
        "\n",
        "#Spallation scaling production equations from Stone 2000:\n",
        "#Input argument l =latitude [deg], pressure_array = elevation [hPa], and Pref = reference production rate at SLHL [atoms/g/yr]\n",
        "def spalProd(l,pressure_array,CRN):\n",
        "    #Interpolation for latitute coefficient. See Table 1 Stone 2000\n",
        "    coeff = np.array([(31.8518, 250.3193, -0.083393, 7.4260E-5, -2.2397E-8),(34.3699, 258.4759, -0.089807, 7.9457E-5, -2.3697E-8),(40.3153, 308.9894, -0.106248, 9.4508E-5, -2.8234E-8),(42.0983, 512.6857, -0.120551, 1.1752E-4, -3.8809E-8), (56.7733, 649.1343, -0.160859, 1.5463E-4, -5.033E-8),(69.0720, 832.4566, -0.199252, 1.9391E-4, -6.3653E-8),(71.8733, 863.1927, -0.207069, 2.0127E-4, -6.6043E-8)])\n",
        "    a_coeff = coeff[:,0]\n",
        "    b_coeff = coeff[:,1]\n",
        "    c_coeff = coeff[:,2]\n",
        "    d_coeff = coeff[:,3]\n",
        "    e_coeff = coeff[:,4]\n",
        "    lat = np.array([0,10,20,30,40,50,60])\n",
        "    a_func = interpolate.interp1d(lat,a_coeff)\n",
        "    b_func = interpolate.interp1d(lat,b_coeff)\n",
        "    c_func = interpolate.interp1d(lat,c_coeff)\n",
        "    d_func = interpolate.interp1d(lat,d_coeff)\n",
        "    e_func = interpolate.interp1d(lat,e_coeff)\n",
        "    Sp = a_func(l)+(b_func(l)*np.exp(-1*pressure_array/150))+(c_func(l)*pressure_array)+(d_func(l)*(pressure_array**2))+(e_func(l)*(pressure_array**3)) #Scaling factor\n",
        "    Pref = [3.84,25.92,11.7]\n",
        "    return 0.978*Sp*Pref[CRN]\n",
        "\n",
        "#Muon scaling production equations from Balco2017\n",
        "@jit(nopython = True)\n",
        "def muProd(baro,CRN):\n",
        "    Pref = [0.0735,0.6764,3.067] #atoms/g/yr\n",
        "    atten = [299.2,288.0,267.8] #hPa\n",
        "    return Pref[CRN]*np.exp((1013.25-baro)/atten[CRN])\n",
        "\n",
        "# Calculate the concentration of muons with depth using a pre-calculated grid of concentrations with depths and erosion rates\n",
        "# from Heisinger scheme if cell was at steady-state\n",
        "# note: this only considers the average pressure of the catchment to simplify\n",
        "def CmuDepthSteadyState(erosion_array,depth):\n",
        "    valid_values = depth > 0.0\n",
        "    rows,cols = np.where(valid_values)\n",
        "\n",
        "    erosion = np.where(np.isnan(depth),np.nan,erosion_array)\n",
        "    erosion_flat = erosion.flatten()[~np.isnan(erosion.flatten())]\n",
        "    depth_flat = depth.flatten()[~np.isnan(depth.flatten())]\n",
        "\n",
        "    Cmu = griddata((Emu_grid.flatten(),Depth_grid.flatten()),Cmu_steadystate.flatten(),(erosion_flat,depth_flat),method='nearest')\n",
        "\n",
        "    Cmu2D = np.full_like(depth,np.nan,dtype='float')\n",
        "    for i,j,val in zip(rows,cols,Cmu2D):\n",
        "        Cmu2D[i][j] = val\n",
        "\n",
        "    return Cmu2D\n",
        "\n",
        "# Calculates concentration with depth using pre-calculated grid  at the landslide reccurence interval\n",
        "# from Heisinger scheme if cell had landslide\n",
        "def CmuDepthLandslide(erosion_array,depth):\n",
        "    valid_values = depth > 0.0\n",
        "    rows,cols = np.where(valid_values)\n",
        "\n",
        "    erosion = np.where(np.isnan(depth),np.nan,erosion_array)\n",
        "    erosion_flat = erosion.flatten()[~np.isnan(erosion.flatten())]\n",
        "    depth_flat = depth.flatten()[~np.isnan(depth.flatten())]\n",
        "\n",
        "    Cmu = griddata((Emu_grid.flatten(),Depth_grid.flatten()),Cmu_landslide.flatten(),(erosion_flat,depth_flat),method='nearest')\n",
        "\n",
        "    Cmu2D = np.full_like(depth,np.nan,dtype='float')\n",
        "    for i,j,val in zip(rows,cols,Cmu2D):\n",
        "        Cmu2D[i][j] = val\n",
        "\n",
        "    return Cmu2D\n",
        "\n",
        "#Muon attenuation length with elevation and erosion rate using pre-calculated grid\n",
        "##note: griddate cannot handle nan values as inputs so need to get rid of them then replace back into 2d array\n",
        "def LeffCalculator(pressure_array,erosion_array,P_grid,E_grid,Leff):\n",
        "\n",
        "    valid_values = erosion_array>0.0\n",
        "    rows,cols = np.where(valid_values)\n",
        "\n",
        "    baro = np.where(np.isnan(erosion_array),np.nan,pressure_array)\n",
        "    baro_flat = baro.flatten()[~np.isnan(baro.flatten())]\n",
        "    erosion_flat = erosion_array.flatten()[~np.isnan(erosion_array.flatten())]\n",
        "    #print(len(baro_flat),len(erosion_flat))\n",
        "    Leffmu = griddata((P_grid.flatten(),E_grid.flatten()),Leff.flatten(),(baro_flat,erosion_flat),method='nearest')\n",
        "\n",
        "    Leff2D = np.full_like(baro,np.nan,dtype='float')\n",
        "    for i,j,val in zip(rows,cols,Leffmu):\n",
        "        Leff2D[i][j] = val\n",
        "\n",
        "    return Leff2D\n",
        "\n",
        "#Concentration if t is inf and at the surface. Exponenital for muon and spallations\n",
        "def surfaceConcentration(pressure_array,erosion_array,landslide_array):\n",
        "    Psp = spalProd(latitude,pressure_array,CRN)\n",
        "    #print(np.nanmean(Psp))\n",
        "    Pmu = muProd(pressure_array,CRN)\n",
        "    #print(np.nanmean(Pmu))\n",
        "    Leffmu = LeffCalculator(pressure_array,erosion_array,P_grid,E_grid,Leff[CRN])\n",
        "    Csp = (Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))*np.exp(-rho*landslide_array/sp_efold)\n",
        "    Cmu = (Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))*np.exp(-rho*landslide_array/Leffmu)\n",
        "    #print(np.nanmean(Csp))\n",
        "    #print(np.nanmean(Cmu))\n",
        "    return Csp+Cmu\n",
        "\n",
        "# Concentration with depth if the concentration was at steady-state\n",
        "@jit(nopython = True)\n",
        "def steadyStateDepth(erosion_array,depth):\n",
        "    Csp_depth = (Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))*np.exp(-rho*depth/sp_efold)\n",
        "    Cmu_depth = (Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))*np.exp(-rho*depth/Leffmu)\n",
        "    #Cmu_depth = CmuDepthSteadyState(erosion_array,depth)\n",
        "    return Csp_depth+Cmu_depth\n",
        "\n",
        "# Concentration with depth if there is inheritance from a previous landslide\n",
        "@jit(nopython = True)\n",
        "def previousLandslideDepth(erosion_array,depth,C_inheritance):\n",
        "    Csp_depth = (Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))*np.exp(-rho*depth/sp_efold)*(1-np.exp(-1*(decayConst[CRN]+(rho*erosion_array/sp_efold))*landslide_reccurence))\n",
        "    Cmu_depth = (Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))*np.exp(-rho*depth/Leffmu)*(1-np.exp(-1*(decayConst[CRN]+(rho*erosion_array/Leffmu))*landslide_reccurence))\n",
        "    #Cmu_depth = CmuDepthLandslide(erosion_array,depth)\n",
        "    return Csp_depth+Cmu_depth+C_inheritance\n",
        "\n",
        "@jit(nopython = True)\n",
        "def concThruTime(erosion_array,C_inheritance,time):\n",
        "    Csp_landslide_time = ((Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))*(1-np.exp(-1*(decayConst[CRN]+(rho*erosion_array/sp_efold))*time)))\n",
        "    Cmu_landslide_time = ((Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))*(1-np.exp(-1*(decayConst[CRN]+(rho*erosion_array/Leffmu))*time)))\n",
        "    return Csp_landslide_time+Cmu_landslide_time+C_inheritance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calculating the concentration through time of n percent of hillslope cells and t residence time.\n",
        "def LandslideTimeCalculator(pressure_array,erosion_array,landslide_array):\n",
        "\n",
        "    C_list = []\n",
        "\n",
        "    C_landslide_surface_list = []\n",
        "\n",
        "    mins = []\n",
        "    maxs = []\n",
        "    q1 = []\n",
        "    q3 = []\n",
        "    medians = []\n",
        "    means = []\n",
        "    whishi =[]\n",
        "    whislo = []\n",
        "\n",
        "    #for total_time:\n",
        "\n",
        "    #Solve for initial steady-state surface concentration\n",
        "    start = datetime.datetime.now()\n",
        "    Csp_0_surface = (Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))\n",
        "    Cmu_0_surface = (Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))\n",
        "    C_0_surface = Csp_0_surface + Cmu_0_surface\n",
        "    #print('Initial surface C:',C_0_surface)\n",
        "    C_0_surface_flat = C_0_surface[~np.isnan(C_0_surface)]\n",
        "    ##Add extra arrays for residence time and multiple by catchment percent\n",
        "    for residence in range(residence_time):\n",
        "        C_0_surface_connected = np.random.choice(C_0_surface_flat,size=int(len(C_0_surface_flat)*catchment_percent),replace=False)\n",
        "        C_list.append(C_0_surface_connected)\n",
        "    end = datetime.datetime.now()\n",
        "    print('calculated initial steady-state surface concentration took:',end-start)\n",
        "    #print('C_list:',C_list)\n",
        "    C_inheritance = np.zeros_like(C_0_surface) #initial inheritance\n",
        "    C_previous_surface = C_0_surface #initiate background concentration\n",
        "\n",
        "    landslide_time = range(t_interval,landslide_reccurence-t_interval+1,t_interval)\n",
        "    total_time = np.zeros_like(C_0_surface) #Make time array\n",
        "    landslide_masks = [np.full_like(C_0_surface,False,dtype=bool)] #initial landslide mask\n",
        "    Csp_0 = np.zeros_like(C_0_surface)\n",
        "    Cmu_0 = np.zeros_like(C_0_surface)\n",
        "\n",
        "    for i in range(0,len(landslide_array)):\n",
        "        print('On landslide number:',i+1)\n",
        "        #print('C_previous surface:',C_previous_surface)\n",
        "        #print('C_inheritance',C_inheritance)\n",
        "        current_landslide_mask = ~np.isnan(landslide_array[i])\n",
        "        #print('current landslide mask:',current_landslide_mask)\n",
        "        total_time = np.where(current_landslide_mask,0,total_time) #start time over where there was a landslide\n",
        "        #print('total_time:',total_time)\n",
        "        #landslide_masks.append(current_landslide_mask)\n",
        "        previous_landslide_masks = np.logical_or.reduce(landslide_masks) #Combine all landslide masks\n",
        "        #print('previous landslide masks:',previous_landslide_masks)\n",
        "        #Calculate the landslide concentration with depth\n",
        "        start = datetime.datetime.now()\n",
        "        depth_arrays = depthSlicer(landslide_array[i],D50)\n",
        "        end = datetime.datetime.now()\n",
        "        print('depth slicer took:',end-start)\n",
        "\n",
        "        #Calculate the concentration of each depth\n",
        "        C_total_depths = []\n",
        "        for depth in depth_arrays:\n",
        "            nan_mask = ~np.isnan(depth)\n",
        "            C_each_depth = np.where(previous_landslide_masks,previousLandslideDepth(erosion_array,depth,C_inheritance),steadyStateDepth(erosion_array,depth))\n",
        "            #print('Each depth:',C_each_depth)\n",
        "            C_total_depths.append(C_each_depth[nan_mask]) #get rid of nans\n",
        "\n",
        "        #memory management\n",
        "        depth_arrays = None\n",
        "\n",
        "        print('Calculated the concentration of each landslide depth')\n",
        "        C_total_depths_merged = list(itertools.chain(*C_total_depths)) #combine all depth concentrations\n",
        "        print('Mean landslide concentration:',np.nanmean(C_total_depths_merged))\n",
        "        #print('Total depths merged:',C_total_depths_merged)\n",
        "        #Make surface concentration where there wasn't landslides then add that to C_list\n",
        "        C_landslide_surface = C_previous_surface[np.isnan(landslide_array[i])]\n",
        "        #print('C_landslide_surface:',C_landslide_surface)\n",
        "        C_landslide_surface_connected = np.random.choice(C_landslide_surface,size=int(len(C_landslide_surface)*catchment_percent),replace=False)\n",
        "        C_list.append(np.concatenate((C_total_depths_merged,C_landslide_surface_connected),axis=None)) #combine surface and landsdie depths\n",
        "        #print('length of C_list:',len(C_list))\n",
        "\n",
        "        #Make the new current surface\n",
        "        C_max_depth = np.where(previous_landslide_masks,previousLandslideDepth(erosion_array,landslide_array[i],C_inheritance),steadyStateDepth(erosion_array,landslide_array[i]))\n",
        "        C_current_surface = np.where(current_landslide_mask,C_max_depth,C_previous_surface)\n",
        "        #print('C_current_surface:',C_current_surface)\n",
        "        C_inheritance = np.where(C_max_depth>0,C_max_depth,C_inheritance) #Make inheritance where there was a landslide\n",
        "        #print('New C_inheritance:',C_inheritance)\n",
        "\n",
        "        #Calculate current landslide surface through time:\n",
        "        for time in landslide_time:\n",
        "            print('On time step:',time)\n",
        "            total_time = np.where(previous_landslide_masks | current_landslide_mask,total_time+t_interval,total_time) #Array for each time step. Time starts over for new landslide\n",
        "            time_mask = total_time>0\n",
        "            C_time_step = np.where(time_mask,concThruTime(erosion_array,C_inheritance,total_time),C_current_surface)\n",
        "            #print('C each time step:',C_time_step)\n",
        "            C_time_step_connected = np.random.choice(C_time_step.flatten(),size=int(len(C_time_step.flatten())*catchment_percent),replace=False)\n",
        "            C_list.append(C_time_step_connected)\n",
        "\n",
        "        #Add current landslide to previous landslides\n",
        "        landslide_masks.append(current_landslide_mask)\n",
        "        previous_landslide_masks = np.logical_or.reduce(landslide_masks)\n",
        "        #Make surface where next landslide starts\n",
        "        C_previous_surface =  np.where(time_mask,concThruTime(erosion_array,C_inheritance,landslide_reccurence),C_current_surface)\n",
        "\n",
        "    #Add sediment mixing by combining the last n arrays\n",
        "    print(\"Loop length of %s\"%(len(C_list)+1-residence_time))\n",
        "    for i in range(0,len(C_list)+1-residence_time):\n",
        "        print(\"At %s\"%(i))\n",
        "        combined_residence_C = C_list[i:i+residence_time]\n",
        "        concantenated_arrays = np.concatenate(combined_residence_C)\n",
        "\n",
        "        min_value = np.nanmin(concantenated_arrays)\n",
        "        max_value = np.nanmax(concantenated_arrays)\n",
        "        median_value = np.nanmedian(concantenated_arrays)\n",
        "        mean_value = np.nanmean(concantenated_arrays)\n",
        "        q1_value = np.nanquantile(concantenated_arrays,0.25)\n",
        "        q3_value = np.nanquantile(concantenated_arrays,0.75)\n",
        "        hi = min(max_value,q3+((q3-q1)*1.5))\n",
        "        lo = max(min_value,q1-((q3-q1)*1.5))\n",
        "\n",
        "        mins.append(min_value)\n",
        "        maxs.append(max_value)\n",
        "        medians.append(median_value)\n",
        "        means.append(mean_value)\n",
        "        q1.append(q1_value)\n",
        "        q3.append(q3_value)\n",
        "        whishi.append(hi)\n",
        "        whislo.append(lo)\n",
        "\n",
        "        #memory management\n",
        "        concatenated_arrays = None\n",
        "\n",
        "    return mins,maxs,q1,q3,medians,means,whishi,whislo"
      ],
      "metadata": {
        "id": "Lf0wbymo449A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXMsMaOapq2u"
      },
      "outputs": [],
      "source": [
        "# Function for calculating the concentration of the landslide cells only through time for ONE landslide\n",
        "# # (use this function if want to know the difference in concentration with a landslide between uniform and slope dependent)\n",
        "def LandslideOnlyTime(pressure_array,erosion_array,landslide_array):\n",
        "    C_list = []\n",
        "\n",
        "    mins = []\n",
        "    maxs = []\n",
        "    q1 = []\n",
        "    q3 = []\n",
        "    medians = []\n",
        "    means = []\n",
        "    whishi = []\n",
        "    whislo = []\n",
        "\n",
        "    landslide_mask = np.isnan(landslide_array)\n",
        "    pressure_array = np.where(landslide_mask,np.nan,pressure_array)\n",
        "    erosion_array = np.where(landslide_mask,np.nan,erosion_array)\n",
        "\n",
        "    #Solve for initial steady-state surface concentration\n",
        "    start = datetime.datetime.now()\n",
        "    Csp_0_surface = (Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))\n",
        "    Cmu_0_surface = (Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))\n",
        "    C_0_surface = Csp_0_surface + Cmu_0_surface\n",
        "    print('Mean surface C',np.nanmean(C_0_surface))\n",
        "    C_list.append(C_0_surface)\n",
        "    #print('Initial surface C:',C_0_surface)\n",
        "    #total_time = np.zeros_like(C_0_surface)\n",
        "    landslide_time = range(t_interval,landslide_reccurence-t_interval+1,t_interval)\n",
        "    depth_arrays = depthSlicer(landslide_array,D50)\n",
        "    end = datetime.datetime.now()\n",
        "    #print('depth slicer took:',end-start)\n",
        "\n",
        "    #Calculate the concentration of each depth\n",
        "    C_total_depths = []\n",
        "    for depth in depth_arrays:\n",
        "        nan_mask = ~np.isnan(depth)\n",
        "        C_each_depth = steadyStateDepth(erosion_array,depth)\n",
        "        print('Each depth:',C_each_depth)\n",
        "        C_total_depths.append(C_each_depth[nan_mask]) #get rid of nans\n",
        "\n",
        "    #print('Calculated the concentration of each landslide depth')\n",
        "    C_total_depths_merged = list(itertools.chain(*C_total_depths)) #combine all depth concentrations\n",
        "    #print('Mean landslide depth',np.nanmean(C_total_depths_merged))\n",
        "    C_list.append(C_total_depths_merged)\n",
        "\n",
        "    depth_arrays = None\n",
        "    #Calculate current landslide surface through time:\n",
        "    C_max_depth = steadyStateDepth(erosion_array,landslide_array)\n",
        "    print('C amx depth',np.nanmean(C_max_depth))\n",
        "    for time in landslide_time:\n",
        "        print('On time step:',time)\n",
        "        C_time_step = concThruTime(erosion_array,C_max_depth,time)\n",
        "        print('Mean each time step',np.nanmean(C_time_step))\n",
        "        C_list.append(C_time_step)\n",
        "\n",
        "\n",
        "    counter = 0\n",
        "    for c in C_list:\n",
        "        counter+=1\n",
        "        print(counter)\n",
        "        min_c = np.nanmin(c)\n",
        "        max_c = np.nanmax(c)\n",
        "        median = np.nanmedian(c)\n",
        "        mean = np.nanmean(c)\n",
        "        q1_c = np.nanquantile(c,0.25)\n",
        "        q3_c = np.nanquantile(c,0.75)\n",
        "        hi = min(max_c,q3+((q3-q1)*1.5))\n",
        "        lo = max(min_c,q1-((q3-q1)*1.5))\n",
        "        mins.append(min_c)\n",
        "        maxs.append(max_c)\n",
        "        q1.append(q1_c)\n",
        "        q3.append(q3_c)\n",
        "        medians.append(median)\n",
        "        means.append(mean)\n",
        "        whishi.append(hi)\n",
        "        whislo.append(lo)\n",
        "\n",
        "\n",
        "    return mins,maxs,q1,q3,medians,means,whishi,whislo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for calculating the concentration in the channel through time only considering the landslide cells as input in every time step\n",
        "def LandslideOnlyTimeCalculator(pressure_array,erosion_array,landslide_array):\n",
        "    C_list = []\n",
        "\n",
        "    C_landslide_surface_list = []\n",
        "\n",
        "    mins = []\n",
        "    maxs = []\n",
        "    q1 = []\n",
        "    q3 = []\n",
        "    medians = []\n",
        "    means = []\n",
        "    whishi = []\n",
        "    whislo = []\n",
        "\n",
        "    #for total_time:\n",
        "\n",
        "    #Solve for initial steady-state surface concentration\n",
        "    start = datetime.datetime.now()\n",
        "    Csp_0_surface = (Psp/(decayConst[CRN]+(rho*erosion_array/sp_efold)))\n",
        "    Cmu_0_surface = (Pmu/(decayConst[CRN]+(rho*erosion_array/Leffmu)))\n",
        "    C_0_surface = Csp_0_surface + Cmu_0_surface\n",
        "    #print('Initial surface C:',C_0_surface)\n",
        "    C_0_surface_flat = C_0_surface[~np.isnan(C_0_surface)]\n",
        "    ##Add extra arrays for residence time and multiple by catchment percent\n",
        "\n",
        "    for residence in range(residence_time):\n",
        "        C_0_surface_connected = np.random.choice(C_0_surface_flat,size=int(len(C_0_surface_flat)*catchment_percent),replace=False)\n",
        "        C_list.append(C_0_surface_connected)\n",
        "    end = datetime.datetime.now()\n",
        "    print('calculated initial steady-state surface concentration took:',end-start)\n",
        "    #print('C_list:',C_list)\n",
        "\n",
        "    C_inheritance = np.zeros_like(C_0_surface) #initial inheritance\n",
        "    C_previous_surface = C_0_surface #initiate background concentration\n",
        "\n",
        "    landslide_time = range(t_interval,landslide_reccurence-t_interval+1,t_interval)\n",
        "    total_time = np.zeros_like(C_0_surface) #Make time array\n",
        "    landslide_masks = [np.full_like(C_0_surface,False,dtype=bool)] #initial landslide mask\n",
        "    Csp_0 = np.zeros_like(C_0_surface)\n",
        "    Cmu_0 = np.zeros_like(C_0_surface)\n",
        "\n",
        "    for i in range(0,len(landslide_array)):\n",
        "        print('On landslide number:',i+1)\n",
        "        #print('C_previous surface:',C_previous_surface)\n",
        "        #print('C_inheritance',C_inheritance)\n",
        "        current_landslide_mask = ~np.isnan(landslide_array[i])\n",
        "        #print('current landslide mask:',current_landslide_mask)\n",
        "        total_time = np.where(current_landslide_mask,0,total_time) #start time over where there was a landslide\n",
        "        #print('total_time:',total_time)\n",
        "        #landslide_masks.append(current_landslide_mask)\n",
        "        previous_landslide_masks = np.logical_or.reduce(landslide_masks) #Combine all landslide masks\n",
        "        #print('previous landslide masks:',previous_landslide_masks)\n",
        "        #Calculate the landslide concentration with depth\n",
        "        start = datetime.datetime.now()\n",
        "        depth_arrays = depthSlicer(landslide_array[i],D50)\n",
        "        end = datetime.datetime.now()\n",
        "        print('depth slicer took:',end-start)\n",
        "\n",
        "        #Calculate the concentration of each depth\n",
        "        C_total_depths = []\n",
        "        for depth in depth_arrays:\n",
        "            nan_mask = ~np.isnan(depth)\n",
        "            C_each_depth = np.where(previous_landslide_masks,previousLandslideDepth(erosion_array,depth,C_inheritance),steadyStateDepth(erosion_array,depth))\n",
        "            #print('Each depth:',C_each_depth)\n",
        "            C_total_depths.append(C_each_depth[nan_mask]) #get rid of nans\n",
        "\n",
        "        #memory management\n",
        "        depth_arrays = None\n",
        "\n",
        "        print('Calculated the concentration of each landslide depth')\n",
        "        C_total_depths_merged = list(itertools.chain(*C_total_depths)) #combine all depth concentrations\n",
        "        print('Mean landslide concentration:',np.nanmean(C_total_depths_merged))\n",
        "        #print('Total depths merged:',C_total_depths_merged)\n",
        "\n",
        "        #Make the new current surface\n",
        "        C_max_depth = np.where(previous_landslide_masks,previousLandslideDepth(erosion_array,landslide_array[i],C_inheritance),steadyStateDepth(erosion_array,landslide_array[i]))\n",
        "        C_current_surface = np.where(current_landslide_mask,C_max_depth,C_previous_surface)\n",
        "        #print('C_current_surface:',C_current_surface)\n",
        "        C_inheritance = np.where(C_max_depth>0,C_max_depth,C_inheritance) #Make inheritance where there was a landslide\n",
        "        #print('New C_inheritance:',C_inheritance)\n",
        "\n",
        "        #Calculate current landslide surface through time. Cells contributing into the channel are only cells that have had a landslide:\n",
        "        for time in landslide_time:\n",
        "            print('On time step:',time)\n",
        "            total_time = np.where(previous_landslide_masks | current_landslide_mask,total_time+t_interval,total_time) #Array for each time step. Time starts over for new landslide\n",
        "            time_mask = total_time>0\n",
        "            C_time_step = np.where(time_mask,concThruTime(erosion_array,C_inheritance,total_time),C_current_surface)\n",
        "            #print('C each time step:',C_time_step)\n",
        "            C_time_step_connected = np.where(current_landslide_mask,C_time_step,np.nan)\n",
        "            C_list.append(C_time_step_connected[~np.isnan(C_time_step_connected)])\n",
        "\n",
        "        #Add current landslide to previous landslides\n",
        "        landslide_masks.append(current_landslide_mask)\n",
        "        previous_landslide_masks = np.logical_or.reduce(landslide_masks)\n",
        "        #Make surface where next landslide starts\n",
        "        C_previous_surface =  np.where(time_mask,concThruTime(erosion_array,C_inheritance,landslide_reccurence),C_current_surface)\n",
        "\n",
        "    #Add sediment mixing by combining the last n arrays\n",
        "    print(\"Loop length of %s\"%(len(C_list)+1-residence_time))\n",
        "    for i in range(0,len(C_list)+1-residence_time):\n",
        "        print(\"At %s\"%(i))\n",
        "        combined_residence_C = C_list[i:i+residence_time]\n",
        "        concantenated_arrays = np.concatenate(combined_residence_C)\n",
        "\n",
        "        min_value = np.nanmin(concantenated_arrays)\n",
        "        max_value = np.nanmax(concantenated_arrays)\n",
        "        median_value = np.nanmedian(concantenated_arrays)\n",
        "        mean_value = np.nanmean(concantenated_arrays)\n",
        "        q1_value = np.nanquantile(concantenated_arrays,0.25)\n",
        "        q3_value = np.nanquantile(concantenated_arrays,0.75)\n",
        "        hi = min(max_value,q3+((q3-q1)*1.5))\n",
        "        lo = max(min_value,q1-((q3-q1)*1.5))\n",
        "\n",
        "        mins.append(min_value)\n",
        "        maxs.append(max_value)\n",
        "        medians.append(median_value)\n",
        "        means.append(mean_value)\n",
        "        q1.append(q1_value)\n",
        "        q3.append(q3_value)\n",
        "        whishi.append(hi)\n",
        "        whislo.append(lo)\n",
        "\n",
        "        #memory management\n",
        "        concatenated_arrays = None\n",
        "\n",
        "    return mins,maxs,q1,q3,medians,means,whishi,whislo"
      ],
      "metadata": {
        "id": "2EP-pMs90x66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0ahwWmsJLH5",
        "outputId": "44d9c1d4-222c-4133-dac9-3366188f8cb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kowhai\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    ######################################\n",
        "    # Paramters for production functions #\n",
        "    ######################################\n",
        "\n",
        "    CRNs = ['Be10','Al26','C14']\n",
        "    halfLife = [1.36e6, 7.17e5, 5730] #Half life\n",
        "    decayConst = np.log(2)/halfLife #Decay constant\n",
        "    fc = [0.704, 0.296, 0.704] #chemical compound factor (Heisinger 2002b)\n",
        "    fd = [0.1828, 0.6559, 0.1828] #fraction of muons stopped by oxygen and absorbed by the nucleus before decay of muon (Heisinger 2002b)\n",
        "    fstar = [0.0043, 0.022, 0.137] #Probability for particle emission to the radionuclide (Heisinger 2002b)\n",
        "    sigmaGeV = [8.6e-29/(190**0.75), 1.41e-27/(190**0.75), 0.45e-27/(190**0.75)] # Muon interaction cross-section for the reaction that produces nuclide i (Table 1 Heisinger 2002a). 10Be updated in v2.2 Balco 2008\n",
        "    Nnuclei = [2.006e22, 1.003e22, 2.006e22] #number density of atoms in target element\n",
        "    sp_efold = 160 #effective attenuation length into rock [g/cm2]\n",
        "\n",
        "    #For indexing\n",
        "    Be10 = 0\n",
        "    Al26 = 1\n",
        "    C14 = 2\n",
        "    slope_depend = 0\n",
        "    uniform = 1\n",
        "    all_landslides = 0\n",
        "    connected = 1\n",
        "    #reference production rate in atoms/g/yr for spallation production. For 10Be we use number from Putnam et al, C14 from Schaefer et al 2014. We use a 6.75 ratio for 26/10.\n",
        "    Pref = [3.84,25.92, 11.7]\n",
        "    Pref_unc = [0.08,0.08,0.9]\n",
        "\n",
        "    ##############\n",
        "    #USER INPUTS:#\n",
        "    ##############\n",
        "\n",
        "    ## Which CRN would you like to model?\n",
        "    CRN = Be10 #Option are 'Be10','Al26', or 'C14'. Can only do one at a time.\n",
        "\n",
        "    # ##Which erosion type would you like to model?\n",
        "    eType = slope_depend #options are 'uniform, 'slope_depend' npo quotes please\n",
        "\n",
        "    ## Percent of the catchment is contributing to the river?\n",
        "    catchment_percents = [0.95,0.5,0.05]\n",
        "\n",
        "    ## Maximum run time (years)?\n",
        "    tmax = 5\n",
        "\n",
        "    ## Residence time of sediment in the river? (years)\n",
        "    residence_times = [1,5,10]\n",
        "\n",
        "    ## Landslide reccurence interval (years)\n",
        "    landslide_reccurence = 500\n",
        "\n",
        "    ## Time interval model runs in (years)\n",
        "    t_interval = 1\n",
        "\n",
        "    ## Would you like all landslides or connected landslides?\n",
        "    landslideType = all_landslides #options are all_landslides or connected\n",
        "\n",
        "    rho = 2.65 #rock density [g/cm2]\n",
        "    latitude = 42 #[deg]\n",
        "    D50 = 100 #the D50 to slice the landslide depth [cm]\n",
        "\n",
        "    #DEM_name = 'Hapuku'\n",
        "    DEM_name = 'Kowhai'\n",
        "    print(DEM_name)\n",
        "    # #Where rasters are saved\n",
        "    #main_folder = '/Users/home/fiemandi/CRNC/'\n",
        "    #elevation_folder = '/Users/home/fiemandi/CRNC/Topo_Analysis/'\n",
        "    #erosion_folder = '/Users/home/fiemandi/CRNC/Surface_Conc_Erate_Maker/'\n",
        "\n",
        "    #elevation_folder = r'C:\\\\Users\\\\cbradbury\\\\Documents\\\\Topo_Analysis\\\\'\n",
        "    #erosion_folder = r'C:\\\\Users\\\\cbradbury\\\\Documents\\\\Erosion_Rasters\\\\'\n",
        "\n",
        "    elevation_folder = '/content/drive/MyDrive/Topo_Analysis/'\n",
        "    erosion_folder = '/content/drive/MyDrive/Erosion_Rasters/'\n",
        "    landslide_folder = '/content/drive/MyDrive/Model Landslides/'\n",
        "    rasterformat = '.tif'\n",
        "    uniform_erosion = 0.246 if DEM_name == 'Hapuku' else 0.288"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    if DEM_name == 'Kowhai':\n",
        "        landslide_rasters = ['Kowhai_ModelLandslides_0','Kowhai_ModelLandslides_1','Kowhai_ModelLandslides_2','Kowhai_ModelLandslides_3','Kowhai_ModelLandslides_4','Kowhai_ModelLandslides_5','Kowhai_ModelLandslides_6','Kowhai_ModelLandslides_7','Kowhai_ModelLandslides_8','Kowhai_ModelLandslides_9']\n",
        "    if DEM_name == 'Hapuku':\n",
        "        landslide_rasters = ['Hapuku_ModelLandslides_0','Hapuku_ModelLandslides_1','Hapuku_ModelLandslides_2','Hapuku_ModelLandslides_3','Hapuku_ModelLandslides_4','Hapuku_ModelLandslides_5','Hapuku_ModelLandslides_6','Hapuku_ModelLandslides_7','Hapuku_ModelLandslides_8','Hapuku_ModelLandslides_9']\n",
        "    multiple_distributions = [gdal_open(landslide_raster+rasterformat,landslide_folder)*100 for landslide_raster in landslide_rasters]"
      ],
      "metadata": {
        "id": "hK3JZyew11Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8X5acc9qLvGw",
        "outputId": "97ebac98-b547-455e-af07-5598ed2a9ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kowhai_K0.004_Sc9.90_SlopeErosion\n",
            "Kowhai_SourceAreas\n",
            "Kowhai_Masked_Wshd\n",
            "True\n",
            "opened all input rasters\n"
          ]
        }
      ],
      "source": [
        "    # #Open rasters to convert to arrays and covert erosion and depth from m to cm\n",
        "    erosion_raster = 'Hapuku_K0.003_Sc9.89_SlopeErosion' if DEM_name == 'Hapuku' else 'Kowhai_K0.004_Sc9.90_SlopeErosion'\n",
        "    landslide_rasters = DEM_name+'_SourceAreas'#,DEM_name+'_SourceAreas_Con']\n",
        "    elevation_raster = f'{DEM_name}_Masked_Wshd'\n",
        "    print(erosion_raster)\n",
        "    print(landslide_rasters)\n",
        "    print(elevation_raster)\n",
        "\n",
        "    check_file = os.path.exists(elevation_folder+landslide_rasters+rasterformat)\n",
        "    print(check_file)\n",
        "\n",
        "    KaikouraEQ_landslideArray = gdal_open(landslide_rasters+rasterformat,elevation_folder)*100\n",
        "\n",
        "    elevationArray = gdal_open(elevation_raster+rasterformat,elevation_folder)\n",
        "\n",
        "    erosionArrays = [gdal_open(erosion_raster+rasterformat,erosion_folder),np.where(np.isnan(elevationArray),np.nan,np.full_like(elevationArray,uniform_erosion))]\n",
        "    erosionArray = erosionArrays[eType]\n",
        "    #erosionArray = np.where(np.isnan(elevationArray),np.nan,np.full_like(elevationArray,uniform_erosion))\n",
        "    #print(len(elevationArray.flatten()[~np.isnan(elevationArray.flatten())]),len(erosionArray.flatten()[~np.isnan(erosionArray.flatten())]))\n",
        "\n",
        "    print('opened all input rasters')\n",
        "\n",
        "    #Pre-calculated Leff grid from Balco 2016 for muon efolding length\n",
        "    #Erate in cm/yr\n",
        "    Erate = [0,0.0001,0.000158489,0.000251189,0.000398107,0.000630957,0.001,0.001584893,0.002511886,0.003981072,0.006309573,0.01,0.015848932,0.025118864,0.039810717,0.063095734,0.1,0.158489319,0.251188643,0.398107171,0.630957344,1]\n",
        "    Pressure = [450,480,510,540,570,600,630,660,690,720,750,780,810,840,870,900,930,960,990,1020]\n",
        "\n",
        "    P_grid,E_grid = np.meshgrid(Pressure,Erate)\n",
        "\n",
        "    #Rows are erosion rate, columns are pressure\n",
        "    Leff26 = np.genfromtxt(elevation_folder+'Leff26.csv',delimiter=',')\n",
        "    Leff10 = np.genfromtxt(elevation_folder+'Leff10.csv',delimiter=',')\n",
        "    #Leff26 = np.genfromtxt(r'C:\\\\Users\\\\cbradbury\\\\Documents\\\\Topo_Analysis\\\\Leff26.csv',delimiter=',')\n",
        "    #Leff10 = np.genfromtxt(r'C:\\\\Users\\\\cbradbury\\\\Documents\\\\Topo_Analysis\\\\Leff10.csv',delimiter=',')\n",
        "    Leff = [Leff10,Leff26,Leff10] #Greg didn't have the C14 grid so assuming it is equal to Be10 for now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZNPFtLJNZEh",
        "outputId": "adb04a87-2fc7-477f-ee68-d5f71026939e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "solved for production stuff\n"
          ]
        }
      ],
      "source": [
        "    ################\n",
        "    # Calculations #\n",
        "    ################\n",
        "\n",
        "    #Convert elevation [m] (2D array) barometric pressure [hPa] and atmospheric pressure [g/cm2]\n",
        "    pressureArray, atDepth = h_units(elevationArray)\n",
        "\n",
        "    landslideArrays = KaikouraEQ_landslideArray\n",
        "    # pressureArray = np.array([[100,100,100],[200,200,200],[300,300,300]])\n",
        "    # erosionArray = np.array([[0.1,0.1,0.1],[0.01,0.01,0.01],[0.01,0.01,0.01]])\n",
        "    # landslideArray = [np.array([[300,300,np.nan],[300,300,100],[np.nan,np.nan,100]]),np.array([[400,400,np.nan],[np.nan,np.nan,np.nan],[200,200,100]])]\n",
        "\n",
        "    #Solve for surface spallation and muon production:\n",
        "    #Psp = spalProd(latitude,pressureArray,CRN)\n",
        "    #Pmu = muProd(pressureArray,CRN)\n",
        "    #Leffmu = LeffCalculator(pressureArray,erosionArray,P_grid,E_grid,Leff[CRN])\n",
        "    print('solved for production stuff')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    total_catchment_cells = np.count_nonzero(elevationArray[~np.isnan(elevationArray)],axis=None)\n",
        "    total_landslide_cells = np.count_nonzero(landslideArrays[~np.isnan(landslideArrays)],axis=None)\n",
        "    flux_5 = total_catchment_cells*0.05\n",
        "    flux_50 = total_catchment_cells*0.5\n",
        "    flux_95 = total_catchment_cells*0.95\n",
        "    print(flux_5,flux_50,flux_95)\n",
        "    print(total_landslide_cells)\n",
        "    print(total_catchment_cells)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x85s2jyoKmzs",
        "outputId": "3d2e3721-9b92-4bfa-b8c8-fd2be2cc8be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2759518.1 27595181.0 52430843.9\n",
            "2608608\n",
            "55190362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    #\"\"\"\n",
        "    #loading previously calculated muon data\n",
        "    if DEM_name == \"Hapuku\":\n",
        "        depth_grid = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Hapuku_Depths.csv',delimiter=',')\n",
        "        erosion_grid = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Hapuku_Erosions.csv',delimiter=',')\n",
        "        muon_grid = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Hapuku_SlopeDependent_Muons.csv',delimiter=',')\n",
        "        muon_grid_LS = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Hapuku_SlopeDependent_Muons_LS.csv',delimiter=',')\n",
        "\n",
        "    if DEM_name == \"Kowhai\":\n",
        "        depth_grid = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Kowhai_Depths.csv',delimiter=',')\n",
        "        erosion_grid = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Kowhai_Erosions.csv',delimiter=',')\n",
        "        muon_grid = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Kowhai_SlopeDependent_Muons.csv',delimiter=',')\n",
        "        muon_grid_LS = np.genfromtxt(r'/content/drive/MyDrive/Colab Notebooks/Kowhai_SlopeDependent_Muons_LS.csv',delimiter=',')\n",
        "    #\"\"\""
      ],
      "metadata": {
        "id": "TXgtRFZAwjVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVqojb59RKaZ",
        "outputId": "28a8cde4-2b50-4d9c-ce93-ca852ee50fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean surface C 2866.137345850908\n",
            "C amx depth 277.30091330806334\n",
            "On time step: 1\n",
            "Mean each time step 287.435977346812\n",
            "On time step: 2\n",
            "Mean each time step 297.5302605380225\n",
            "On time step: 3\n"
          ]
        }
      ],
      "source": [
        "    mins,maxs,q1,q3,medians,means,whishi,whislo = LandslideOnlyTime(pressureArray,erosionArray,KaikouraEQ_landslideArray[0])\n",
        "    time_list = [x*t_interval for x in range(0,len(mins),1)]\n",
        "    C_data = {\n",
        "        'Mins':mins,\n",
        "        'Maxs': maxs,\n",
        "        'Q1': q1,\n",
        "        'Q3': q3,\n",
        "        'Low_Whisk':whislo,\n",
        "        'Up_Whisk':whishi,\n",
        "        'Medians':medians,\n",
        "        'Means':means,\n",
        "        'Time':time_list\n",
        "        }\n",
        "\n",
        "    output_name = DEM_name+'_'+CRNs[CRN]+'_UniformE.csv' if eType == uniform else DEM_name+'_'+CRNs[CRN]+'_SlopeE.csv'\n",
        "    pd.DataFrame(C_data).to_csv(output_name,index=False)#rename this a better name\n",
        "    print(\"made csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKBpqa_1pq2w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTGkQRdupq2w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O4eJMJ7pq2w"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACShJp9Npq2w"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}