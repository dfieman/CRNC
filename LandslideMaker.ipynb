{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMciPyPiwAmU6XsxY6kZNF1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "232zMd17DZYU"
      },
      "outputs": [],
      "source": [
        "from multiprocessing import process\n",
        "import sys\n",
        "import math\n",
        "import numpy as np\n",
        "import itertools\n",
        "from scipy.special import gamma, factorial\n",
        "from matplotlib import pyplot as plt\n",
        "from itertools import product\n",
        "import random\n",
        "from osgeo import gdal\n",
        "import datetime\n",
        "import time\n",
        "import multiprocessing\n",
        "from functools import partial\n",
        "import numba\n",
        "from numba import jit\n",
        "#from line_profiler import LineProfiler\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to filter through the slope to only use values greater than or equal to the critical slope value for failure. Outputs the index of where the values are.\n",
        "@jit(nopython = True)\n",
        "def slopeFilter(slopeArray,slope_value):\n",
        "    rows = []\n",
        "    cols = []\n",
        "    for i in range(0,np.shape(slopeArray)[0]):\n",
        "        for j in range(0,np.shape(slopeArray)[1]):\n",
        "            if slopeArray[i][j] >= slope_value:\n",
        "                rows.append(i)\n",
        "                cols.append(j)\n",
        "    return np.array(rows),np.array(cols)\n",
        "\n",
        "# Function for finding landslide placements. Finds where landslide can fit on the boolean slope array by slicing slope array. If all values are true (>= critical value),\n",
        "# then the row and column of the upper left hand corner are saved. Then randomly choses the row and column where the landslide should be placed.\n",
        "# Outputs a list of data: [row, columns, landslide length, landslide depth]\n",
        "@jit(nopython = True)\n",
        "def landslideMasker(lslength,lsdepth,lsfreq,slopeDEM,slopeRows,slopeCols):\n",
        "    row = []\n",
        "    col = []\n",
        "    lsdata = []\n",
        "    for i in range (0,len(slopeRows)):\n",
        "        if slopeRows[i]+lslength <= np.shape(slopeDEM)[0] and slopeCols[i]+lslength <= np.shape(slopeDEM)[1]:\n",
        "            landslideMatrix = slopeDEM[slopeRows[i]:slopeRows[i]+lslength, slopeCols[i]:slopeCols[i]+lslength]\n",
        "            if np.all(landslideMatrix):\n",
        "                row.append(slopeRows[i])\n",
        "                col.append(slopeCols[i])\n",
        "    if len(row) > 0:\n",
        "        row_index = np.arange(len(row))#for randomising which index to use for landslide\n",
        "        rand_index = np.random.choice(row_index,size=lsfreq)\n",
        "        #print(lslength,rand_index)\n",
        "\n",
        "        for lsindex in rand_index:\n",
        "            lsdata.append([row[lsindex],col[lsindex],lslength,lsdepth])\n",
        "    else:\n",
        "        print(\"couldn't find a landslide spot\")\n",
        "    return lsdata\n",
        "\n",
        "\n",
        "# Function to make sure landslides are not overlapping. For each landslide checks to see if any values are FALSE.\n",
        "# If FALSE values then places landslide in 'failed_data' list. If all values are TRUE places\n",
        "# in 'passed_data' list then changes values to FALSE so next lanslide cannot be placed there\n",
        "def landslideChecker(slope_array_boolean,landslideData):\n",
        "    passed_data = []\n",
        "    failed_data = []\n",
        "\n",
        "    for currentLandslide in landslideData:\n",
        "        originPoint = [int(currentLandslide[0]),int(currentLandslide[1])]\n",
        "        lslength = int(currentLandslide[2])\n",
        "        lsdepth = currentLandslide[3]\n",
        "        working_mask = slope_array_boolean[originPoint[0]:originPoint[0]+lslength,originPoint[1]:originPoint[1]+lslength]\n",
        "        if not np.all(working_mask):\n",
        "            failed_data.append(np.array([originPoint[0],originPoint[1],lslength,lsdepth]))\n",
        "            continue\n",
        "        #if np.all(working_mask):\n",
        "        slope_array_boolean[originPoint[0]:originPoint[0]+lslength,originPoint[1]:originPoint[1]+lslength] = False\n",
        "        passed_data.append(np.array([originPoint[0],originPoint[1],lslength,lsdepth]))\n",
        "        #print(slope_array_boolean.sum())\n",
        "    return passed_data,failed_data,slope_array_boolean\n",
        "\n",
        "# Function for placing landslides on an array the same shape as DEM.\n",
        "def landslideBurner(landslideData,depth_array):\n",
        "    for currentLandslide in landslideData:\n",
        "        originPoint = [int(item) for item in currentLandslide[0:2]]\n",
        "        lslength = int(currentLandslide[2])\n",
        "        lsdepth = currentLandslide[3]\n",
        "        depth_array[originPoint[0]:originPoint[0]+lslength, originPoint[1]:originPoint[1]+lslength] = lsdepth\n",
        "    return depth_array\n",
        "\n",
        "#Probability density function for making landslide distribution:\n",
        "def pdf(x):\n",
        "    return (1/(a*gamma(p)))*((a/(x-s))**(p+1))*np.exp(-1*a/(x-s))\n",
        "\n",
        "def RasterMaker(array,output_file,reference_tiff,nodata):\n",
        "    driver = gdal.GetDriverByName('GTiff')\n",
        "    raster = driver.CreateCopy(output_file, reference_tiff, strict=0, options=[\"TILED=YES\",\"COMPRESS=PACKBITS\"])\n",
        "    raster.GetRasterBand(1).WriteArray(array)\n",
        "    raster.GetRasterBand(1).SetNoDataValue(nodata)\n",
        "    raster = None"
      ],
      "metadata": {
        "id": "OqHjRvVGD59L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    ################\n",
        "    # User inputs: #\n",
        "    ################\n",
        "\n",
        "    #where input DEMs are saved\n",
        "    folder = \"/Users/home/fiemandi/CRNC/Topo_Analysis/\"\n",
        "\n",
        "    #DEM_name = 'Hapuku'\n",
        "    #DEM_name = 'Kowhai'\n",
        "    DEM_name = 'Baton'\n",
        "    #How many landslide rasters would you like to make?\n",
        "    N_landslides = 10\n",
        "\n",
        "    #magnitude of event based on Malamud et al. (2004). USe if no idea how many landslides to create\n",
        "    mag = 4\n",
        "    #minimum landslide area (m^2)\n",
        "    areals_min = 0.8 if DEM_name == 'Hapuku' else 8.1\n",
        "    #maximum landslide area (m^2)\n",
        "    areals_max = 527574.8#if DEM_name == 'Hapuku' else 165227.5\n",
        "    #total number of landslides in the catchment\n",
        "    #N_ls = 1554 if DEM_name == 'Hapuku' else 2823\n",
        "    N_ls = 8000\n",
        "\n",
        "    #Scalars for pdf and volume-area. See Malamud et al 2004\n",
        "    a = 241.46 #scalar controlling lcoation of maximum probability\n",
        "    s = -31.65 #scalar controlling exponential decay for small landslides\n",
        "    p = 0.89 #scalar controlling power-law decay for medium and large landslides\n",
        "    e = 0.08 #scalar used for volume (Malamud et al. 2004)\n",
        "    y = 1.45 #exponent used for volume (Malamud et al. 2004)\n",
        "\n",
        "    #minimum slope value where landslides can occur in degrees\n",
        "    criticalSlope_degrees = 20.0\n",
        "\n",
        "    #########\n",
        "    # Main: #\n",
        "    #########\n",
        "\n",
        "    bins = math.isqrt(N_ls) #number of landslide areas/bins in the catchment\n",
        "    ls_areas = np.geomspace(areals_min, areals_max, bins) #chosen landslide areas based on the max and min area and how many bins you want equal in logspace in m^2\n",
        "\n",
        "    criticalSlope = math.tan(criticalSlope_degrees*(math.pi/180)) #convert slope degrees\n",
        "\n",
        "    #Open slope array and make a landslide array\n",
        "    slope_raster = gdal.Open(folder+DEM_name+'_r6_Slope.tif')\n",
        "    nodata = slope_raster.GetRasterBand(1).GetNoDataValue()\n",
        "    xres,yres = slope_raster.GetGeoTransform()[1:6:4]\n",
        "    slope_array = slope_raster.GetRasterBand(1).ReadAsArray()\n",
        "    print(f'opened {DEM_name} slope raster from DiffusionMaker')\n",
        "    slope_array[slope_array==nodata]=np.nan\n",
        "\n",
        "    #Getting the bin width of each landslide area:\n",
        "    bin_width = []\n",
        "\n",
        "    for i in range(0,(np.shape(ls_areas)[0]-1)):\n",
        "        width = ls_areas[i+1]-ls_areas[i]\n",
        "        bin_width.append(width)\n",
        "\n",
        "    lsAreas = np.delete(ls_areas,0)#Each landslide area that will be plotted, uses the maximum area of each bin\n",
        "    lsfreq = np.asarray(pdf(lsAreas)*N_ls*bin_width,dtype=\"int\") #Number of landslides in each bin\n",
        "    lslengths = np.sqrt(lsAreas) #Landslides are assummed to be a rectangle. Units in meters\n",
        "    lsdepths = np.round((e*((np.array(lslengths)**2)**y))/(np.array(lslengths)**2),decimals=6) # meters - depth is based on the volume-area scaling in Malamud 2004\n",
        "\n",
        "    # #For plotting pdf with landslide areas.\n",
        "    # xdata = np.linspace(0, 3000000, num = 3000000)\n",
        "    # fig,ax = plt.subplots(figsize=(8*(1/2.54),8*(1/2.54)),dpi=300)\n",
        "    # ax.plot(xdata, pdf(xdata), color = 'indianred',label=r'$pdf(A_{ls,i}) = \\dfrac{1}{a\\Gamma(p)}\\left(\\dfrac{a}{A_{ls,i}-s}\\right)^{p+1}e^{\\frac{-a}{A_{ls,i}-s}}$')\n",
        "    # ax.scatter(lsAreas, pdf(lsAreas), color = 'cornflowerblue',label='Landslide area chosen for model',s=10)\n",
        "    # ax.set_xlabel('Landslide Area m$^{2}$')\n",
        "    # ax.set_ylabel('Probability Density')\n",
        "    # ax.set_yscale('log')\n",
        "    # ax.set_xscale('log')\n",
        "    # for ax in fig.get_axes():\n",
        "        # ax.title.set_size(8)\n",
        "        # ax.xaxis.label.set_size(8)\n",
        "        # ax.yaxis.label.set_size(8)\n",
        "        # ax.tick_params(axis='both',labelsize=6,direction='in',length=2,width=0.5)\n",
        "        # for location in ['left', 'right', 'top', 'bottom']:\n",
        "            # ax.spines[location].set_linewidth(0.5)\n",
        "    # plt.legend(fontsize=7,loc='lower left')\n",
        "    # plt.title(DEM_name,fontsize=9)\n",
        "    # plt.tight_layout()\n",
        "    # plt.savefig(DEM_name+'ModelPDF.png')\n",
        "    # #plt.show()"
      ],
      "metadata": {
        "id": "2dXliz7GD_lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    #Get rid of landslides that have a frequency of zero\n",
        "    maskfreq = np.where(lsfreq>0)\n",
        "    ls_Freq = lsfreq[maskfreq]\n",
        "    print('total number of landslides',np.sum(ls_Freq))\n",
        "    print(\"Frequencies are:\",ls_Freq)\n",
        "    ls_Lengths = np.asarray((lslengths[maskfreq])/xres, dtype=\"int\") #divide landslide length by resolution, units in cells\n",
        "    print(\"Lengths are:\",ls_Lengths)\n",
        "    ls_Depths = lsdepths[maskfreq]\n",
        "    #lsDepths = np.array([10.0,20.0,30.0])\n",
        "    print(\"Depths are:\",ls_Depths)\n",
        "\n",
        "\n",
        "    N_landslide = 0\n",
        "\n",
        "    while N_landslide < N_landslides:\n",
        "\n",
        "        output_file = '/Users/home/fiemandi/CRNC/Landslide_Maker/Model_Landslides/'+f'{DEM_name}_ModelLandslides_{N_landslide}.tif'\n",
        "        print('N_landslide:',N_landslide)\n",
        "\n",
        "        #Initiate the empty landslide array\n",
        "        landslideArray = np.full_like(slope_array,np.nan)\n",
        "\n",
        "        #shuffle the lists so small landslides are not biased\n",
        "        combined_list = list(zip(ls_Freq,ls_Lengths,ls_Depths))\n",
        "        random.shuffle(combined_list)\n",
        "        lsFreq,lsLengths,lsDepths = zip(*combined_list)\n",
        "        lsFreq,lsLengths,lsDepths = list(lsFreq),list(lsLengths),list(lsDepths)\n",
        "\n",
        "        print('Frequencies:',lsFreq)\n",
        "        print('Lengths',lsLengths)\n",
        "        print('Depths',lsDepths)\n",
        "\n",
        "        #Filter throuh slope where values are only greater than critical slope value\n",
        "        slopeRows,slopeCols = slopeFilter(slope_array, criticalSlope)\n",
        "\n",
        "        #Create boolean array for slope\n",
        "        slope_array_boolean = slope_array >= criticalSlope\n",
        "\n",
        "        #Find indexes to put landslides\n",
        "        #Outputs a list of data: [row, col, landslide length, landslide depth] for each landslide area for where it should be placed\n",
        "        cpus = 40\n",
        "        #print(len(slopeRows),len(slopeCols))\n",
        "        start = datetime.datetime.now()\n",
        "        with multiprocessing.Pool(processes=cpus) as cpu_pool:\n",
        "            landslideDataList = cpu_pool.starmap(partial(landslideMasker,\n",
        "                                                slopeDEM = slope_array_boolean, slopeRows = slopeRows, slopeCols = slopeCols),\n",
        "                                                zip(lsLengths,lsDepths,lsFreq))\n",
        "        end = datetime.datetime.now()\n",
        "\n",
        "        landslideData = [item for sublist in landslideDataList for item in sublist]\n",
        "        landslideData = np.array(landslideData)\n",
        "        print('First landslideMasker took:',end-start)\n",
        "        #\"\"\"\n",
        "        #landslideData = np.load('./landslideDataList.npy')\n",
        "        print('length of landslideData',len(landslideData))\n",
        "\n",
        "        #Check to see if landslides overlap\n",
        "        start = datetime.datetime.now()\n",
        "        passedData,failedData,slope_array_boolean = landslideChecker(slope_array_boolean,landslideData)\n",
        "        end = datetime.datetime.now()\n",
        "        print(\"First landslideChecker completed in:\",end-start)\n",
        "        print(\"Passed:\")\n",
        "        print(len(passedData))\n",
        "        print(\"Failed:\")\n",
        "        print(len(failedData))\n",
        "\n",
        "        counter = 0\n",
        "        while (len(failedData) > 0) and counter <= 2:\n",
        "            temp_data = []\n",
        "            start = datetime.datetime.now()\n",
        "            failedLengths = [landslide[2] for landslide in failedData]\n",
        "            failedDepths = [landslide[3] for landslide in failedData]\n",
        "\n",
        "            #make frequencies from lengths\n",
        "            failedLengths,failedFreq = np.unique(failedLengths, return_counts=True)\n",
        "            failedDepths = np.unique(failedDepths)\n",
        "            print('failed lengths:',failedLengths)\n",
        "            print('failed depths:',failedDepths)\n",
        "            print('failed freq:',failedFreq)\n",
        "\n",
        "            if len(failedLengths) < 40:\n",
        "                cpus = len(failedLengths)\n",
        "\n",
        "            with multiprocessing.Pool(processes=cpus) as cpu_pool:\n",
        "                landslideDataList = cpu_pool.starmap(partial(landslideMasker,\n",
        "                                                     slopeDEM = slope_array_boolean, slopeRows = slopeRows, slopeCols = slopeCols),\n",
        "                                                     zip(failedLengths.astype(int),failedDepths,failedFreq))\n",
        "            #print(landslideDataList)\n",
        "            landslideData = [item for sublist in landslideDataList for item in sublist]\n",
        "            print(len(landslideData))\n",
        "            passedData2,failedData,slope_array_boolean = landslideChecker(slope_array_boolean,np.array(landslideData))\n",
        "            end = datetime.datetime.now()\n",
        "            print('Next landslideMaskers took:',end-start)\n",
        "\n",
        "            print(\"length of failed data:\",len(failedData))\n",
        "            print(\"length of passed data:\",len(passedData2))\n",
        "            passedData.extend(passedData2)\n",
        "            counter += 1\n",
        "\n",
        "        print('Number of tries placing failed landslides:',counter)\n",
        "\n",
        "        print('length of failed data after all tries,', len(failedData))\n",
        "\n",
        "        landslideArray = landslideBurner(passedData,landslideArray)\n",
        "        print(\"Created landslide array with passed data\")\n",
        "\n",
        "        if len(failedData)==0:\n",
        "            print(\"placed all landslides\")\n",
        "            RasterMaker(landslideArray,output_file,slope_raster, nodata)\n",
        "            print('made landslide array into raster',output_file)\n",
        "\n",
        "        if (len(failedData) > 0) and counter > 2:\n",
        "            print('still failed landslides but not trying again sorry')\n",
        "            RasterMaker(landslideArray,output_file,slope_raster, nodata)\n",
        "            print('made landslide array into raster',output_file)\n",
        "\n",
        "        N_landslide += 1\n",
        "\n",
        "    sys.exit()"
      ],
      "metadata": {
        "id": "rTyGrC99EPqW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}